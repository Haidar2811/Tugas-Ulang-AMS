{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "df = df.drop(['Sentimen'], axis=1)\ndf.head()\ndf.info()\nfrom ekphrasis.classes.preprocessor import TextPreProcessor\nfrom ekphrasis.classes.tokenizer import SocialTokenizer\nfrom ekphrasis.dicts.emoticons import emoticons\n\ntext_processor = TextPreProcessor(\n    # terms that will be normalized\n    normalize=['email', 'percent', 'money', 'phone', 'user',\n        'time', 'date', 'number'],\n    # terms that will be annotated\n    #annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",'emphasis', 'censored'},\n    annotate={\"hashtag\"},\n    fix_html=True,  # fix HTML tokens\n    \n    # corpus from which the word statistics are going to be used \n    # for word segmentation \n    segmenter=\"twitter\", \n    \n    # corpus from which the word statistics are going to be used \n    # for spell correction\n    corrector=\"twitter\", \n    \n    unpack_hashtags=True,  # perform word segmentation on hashtags\n    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n    spell_correct_elong=False,  # spell correction for elongated words\n    \n    # select a tokenizer. You can use SocialTokenizer, or pass your own\n    # the tokenizer, should take as input a string and return a list of tokens\n    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n    \n    # list of dictionaries, for replacing tokens extracted from the text,\n    # with other expressions. You can pass more than one dictionaries.\n    dicts=[emoticons]\n)\ndef bersih_data(text):\n    return \" \".join(text_processor.pre_process_doc(text))\n\ndef non_ascii(text):\n    return text.encode('ascii', 'replace').decode('ascii')\n    i = 0\nfinal_string = []\ns = \"\"\nfor text in df['Tweet'].values:\n    filteredSentence = []\n    EachReviewText = \"\"\n    proc = bersih_data(proc)\n    #     dst\n    #     dst\n    #     dst\n    EachReviewText = proc\n    final_string.append(EachReviewText)\ndf[\"step01\"] = final_string\n### Tampilkan posisi data terakhir (10 Teratas)\ndf.head(10)\ndf_hapus = df[~df['step01'].str.contains(\" \")]\ndf_hapus.info()\ndf_hapus.head(10)\ndf_new = df[~df.isin(df_hapus)].dropna()\ndf_new.info()\ndf_new\n## Bisa menggunakan nltk \nimport nltk\nfrom nltk.tokenize import word_tokenize \n \ndf_new['tokens'] = df['step01'].apply(word_tokenize_wrapper)\ndf_new.head(10)\nnormalized_word = pd.read_csv('kamus_clean.csv', sep=\",\")\n### dst\ndf_new['final_tokens'] = df_new['tokens'].apply(normalized_term)\ni=0\nfinal_string_tokens = []\nfor text in df_new['final_tokens'].values:\n    EachReviewText = \"\"\n    EachReviewText = ' '.join(text)\n    final_string_tokens.append(EachReviewText)\ndf_new[\"step02\"] = final_string_tokens\ndf_new.head(10)\ndf_new.to_csv('clean_dataset_uts_part01.csv',sep=\";\")\nfrom Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n \nfactory = StopWordRemoverFactory()\n\nmore_stopword = ['sih', 'nya','rt','loh','lah', 'dd', 'mah', 'nye', 'eh', 'ehh', 'ah', 'yang']\n \n# Tambahkan Stopword Baru\n### Tuliskan perintahnya disini ???\n\nstopwords_sastrawi = factory.create_stop_word_remover()\ndf_new['step02'] = df_new['step02'].apply(str)\ndf_new.head()\ni=0\nfinal_string = []\ns = \"\"\nfor sentence in df_new[\"step02\"].values:\n    filteredSentence = []\n    EachReviewText = \"\"\n    st = stopwords_sastrawi.remove(sentence)\n    s = (stemmer.stem(st))\n    filteredSentence.append(s)\n    \n    EachReviewText = ' '.join(filteredSentence)\n    final_string.append(EachReviewText)\ndf_new.loc[:, ('ProcessedText')] = final_string\ndf_new.head()\ndf_new.to_csv('clean_dataset_uts_part02.csv',sep=\";\")\ndataset_feature = df_new['ProcessedText']\ndataset_label = df_new['Emosi']\ndataset_label.value_counts()\ndataset_label.value_counts(normalize=True).mul(100).round(1).astype(str) + '%'\n# Visualizing the target variable\nplt.figure(figsize=(12,8))\nsns.displot(dataset_label, label=f'target, skew: {dataset_label.skew():.2f}')\nplt.legend(loc='best')\nplt.show()\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer()\npositive_low_reviews = df_new[(df_new[\"Emosi\"] == 1)]\nnegative_low_reviews = df_new[(df_new[\"Emosi\"] == -1)]\npositive_high_reviews = df_new[(df_new[\"Emosi\"] == 2)]\nnegative_high_reviews = df_new[(df_new[\"Emosi\"] == -2)]\npositive_low_reviews.head()\nPositive_1_tf_idf_vect = TfidfVectorizer(ngram_range=(1,2))\nPositive_1_tf_idf = Positive_1_tf_idf_vect.fit_transform(positive_low_reviews[\"ProcessedText\"].values)\nPositive_1_tf_idf.shape\nfeatures = Positive_1_tf_idf_vect.get_feature_names()\n\nidfValues = Positive_1_tf_idf_vect.idf_\n\nd = dict(zip(features, 9 - idfValues))\n\nsortedDict = sorted(d.items(), key = lambda d: d[1], reverse = True)\n\nfor i in range(200):\n    print(sortedDict[i])\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plot\ndef PlotWordCloud(frequency):\n    worcloudPlot = WordCloud(background_color=\"white\", width=1500, height=1000)\n    worcloudPlot.generate_from_frequencies(frequencies=frequency)\n    plot.figure(figsize=(15,10))\n    plot.imshow(worcloudPlot, interpolation=\"bilinear\")\n    plot.axis(\"off\")\n    plot.show()\nPlotWordCloud(d)\nmodels = [\n          ('Naive Bayes Multinomial', accuracy_nb_train, accuracy_nb_test),                    \n         ]\npredict = pd.DataFrame(data = models, columns=['Model', 'Training Accuracy', 'Test Accuracy'])\npredict\nmodels_comparison = [\n                        ('Naive Bayes Multinomial', accuracy_nb_test, recall_nb_test, precision_nb_test),                          \n                    ]\ncomparison = pd.DataFrame(data = models_comparison, columns=['Model', 'Accuracy', 'Recall', 'Precision'])\ncomparison\nimport numpy as np\n\nf, axes = plt.subplots(2,1, figsize=(14,10))\n\npredict.sort_values(by=['Training Accuracy'], ascending=False, inplace=True)\n\nsns.barplot(x='Training Accuracy', y='Model', data = predict, palette='Blues_d', ax = axes[0])\n#axes[0].set(xlabel='Region', ylabel='Charges')\naxes[0].set_xlabel('Training Accuracy', size=16)\naxes[0].set_ylabel('Model')\naxes[0].set_xlim(0,1.0)\naxes[0].set_xticks(np.arange(0, 1.1, 0.1))\n\npredict.sort_values(by=['Test Accuracy'], ascending=False, inplace=True)\n\nsns.barplot(x='Test Accuracy', y='Model', data = predict, palette='Greens_d', ax = axes[1])\n#axes[0].set(xlabel='Region', ylabel='Charges')\naxes[1].set_xlabel('Test Accuracy', size=16)\naxes[1].set_ylabel('Model')\naxes[1].set_xlim(0,1.0)\naxes[1].set_xticks(np.arange(0, 1.1, 0.1))\n\nplt.show()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}